{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG & Embeddings & Vector Store\n",
    "\n",
    "## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ \n",
    "\n",
    "1. å¦‚ä½•ç”¨ä½ çš„å‚åŸŸæ•°æ®è¡¥å…… LLM çš„èƒ½åŠ›\n",
    "2. å¦‚ä½•æ„å»ºä½ çš„å‚åŸŸï¼ˆå‘é‡ï¼‰çŸ¥è¯†åº“\n",
    "3. æ­å»ºä¸€å¥—å®Œæ•´ RAG ç³»ç»Ÿéœ€è¦å“ªäº›æ¨¡å—\n",
    "4. æ­å»º RAG ç³»ç»Ÿæ—¶æ›´å¤šçš„æœ‰ç”¨æŠ€å·§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸€ã€ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºçš„ç”Ÿæˆæ¨¡å‹ï¼ˆRAGï¼‰\n",
    "\n",
    "### 1.1 å¤§æ¨¡å‹ç›®å‰å›ºæœ‰çš„å±€é™æ€§\n",
    "\n",
    "1. LLMçš„çŸ¥è¯†ä¸æ˜¯å®æ—¶çš„\n",
    "2. LLM å¯èƒ½ä¸çŸ¥é“ä½ ç§æœ‰çš„é¢†åŸŸ/ä¸šåŠ¡çŸ¥è¯†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ£€ç´¢å¢å¼ºç”Ÿæˆ\n",
    "\n",
    "RAGï¼ˆRetrieval Augmented Generationï¼‰é¡¾åæ€ä¹‰ï¼Œé€šè¿‡**æ£€ç´¢**çš„æ–¹æ³•æ¥å¢å¼º**ç”Ÿæˆæ¨¡å‹**çš„èƒ½åŠ›ã€‚\n",
    "\n",
    "<video src=\"./assets/RAG.mp4\" controls=\"controls\" width=1024px style=\"margin-left: 0px\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>ç±»æ¯”ï¼š</b>ä½ å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹æƒ³è±¡æˆå¼€å·è€ƒè¯•ã€‚è®© LLM å…ˆç¿»ä¹¦ï¼Œå†å›ç­”é—®é¢˜ã€‚\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºŒã€RAGç³»ç»Ÿçš„åŸºæœ¬æ­å»ºæµç¨‹\n",
    "\n",
    "<img src=\"./assets/rag.png\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "æ­å»ºè¿‡ç¨‹ï¼š\n",
    "\n",
    "1. æ–‡æ¡£åŠ è½½ï¼Œå¹¶æŒ‰ä¸€å®šæ¡ä»¶**åˆ‡å‰²**æˆç‰‡æ®µ\n",
    "2. å°†åˆ‡å‰²çš„æ–‡æœ¬ç‰‡æ®µçŒå…¥**æ£€ç´¢å¼•æ“**\n",
    "3. å°è£…**æ£€ç´¢æ¥å£**\n",
    "4. æ„å»º**è°ƒç”¨æµç¨‹**ï¼šQuery -> æ£€ç´¢ -> Prompt -> LLM -> å›å¤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 æ–‡æ¡£çš„åŠ è½½ä¸åˆ‡å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£… pdf è§£æåº“\n",
    "# !pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):\n",
    "    '''ä» PDF æ–‡ä»¶ä¸­ï¼ˆæŒ‰æŒ‡å®šé¡µç ï¼‰æå–æ–‡å­—'''\n",
    "    paragraphs = []\n",
    "    buffer = ''\n",
    "    full_text = ''\n",
    "    # æå–å…¨éƒ¨æ–‡æœ¬\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        # å¦‚æœæŒ‡å®šäº†é¡µç èŒƒå›´ï¼Œè·³è¿‡èŒƒå›´å¤–çš„é¡µ\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "    # æŒ‰ç©ºè¡Œåˆ†éš”ï¼Œå°†æ–‡æœ¬é‡æ–°ç»„ç»‡æˆæ®µè½\n",
    "    lines = full_text.split('\\n')\n",
    "    for text in lines:\n",
    "        if len(text) >= min_line_length:\n",
    "            buffer += (' '+text) if not text.endswith('-') else text.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "\n",
      " Hugo Touvronâˆ— Louis Martinâ€  Kevin Stoneâ€  Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialomâˆ—\n",
      "\n",
      " GenAI, Meta\n",
      "\n",
      " In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based onour human evaluations for helpfulness and safety, may be a suitable substitute for closed source models. We provide a detailed description of our approach to ï¬ne-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paragraphs = extract_text_from_pdf(\"llama2.pdf\", min_line_length=10)\n",
    "\n",
    "for para in paragraphs[:4]:\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LLMæ¥å£å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), verbose=True)  # è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4o\"):\n",
    "    '''å°è£… openai æ¥å£'''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # æ¨¡å‹è¾“å‡ºçš„éšæœºæ€§ï¼Œ0 è¡¨ç¤ºéšæœºæ€§æœ€å°\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prompt æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt_template, **kwargs):\n",
    "    '''å°† Prompt æ¨¡æ¿èµ‹å€¼'''\n",
    "    inputs = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            val = '\\n\\n'.join(v)\n",
    "        else:\n",
    "            val = v\n",
    "        inputs[k] = val\n",
    "    return prompt_template.format(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººã€‚\n",
    "ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n",
    "\n",
    "å·²çŸ¥ä¿¡æ¯:\n",
    "{context} # æ£€ç´¢å‡ºæ¥çš„åŸå§‹æ–‡æ¡£\n",
    "\n",
    "ç”¨æˆ·é—®ï¼š\n",
    "{query} # ç”¨æˆ·çš„æé—®\n",
    "\n",
    "å¦‚æœå·²çŸ¥ä¿¡æ¯ä¸åŒ…å«ç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆï¼Œæˆ–è€…å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›å¤\"æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜\"ã€‚\n",
    "è¯·ä¸è¦è¾“å‡ºå·²çŸ¥ä¿¡æ¯ä¸­ä¸åŒ…å«çš„ä¿¡æ¯æˆ–ç­”æ¡ˆã€‚\n",
    "è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰ã€å‘é‡æ£€ç´¢\n",
    "\n",
    "### 3.1 ä»€ä¹ˆæ˜¯å‘é‡\n",
    "\n",
    "å‘é‡æ˜¯ä¸€ç§æœ‰å¤§å°å’Œæ–¹å‘çš„æ•°å­¦å¯¹è±¡ã€‚å®ƒå¯ä»¥è¡¨ç¤ºä¸ºä»ä¸€ä¸ªç‚¹åˆ°å¦ä¸€ä¸ªç‚¹çš„æœ‰å‘çº¿æ®µã€‚ä¾‹å¦‚ï¼ŒäºŒç»´ç©ºé—´ä¸­çš„å‘é‡å¯ä»¥è¡¨ç¤ºä¸º $(x,y)$ï¼Œè¡¨ç¤ºä»åŸç‚¹ $(0,0)$ åˆ°ç‚¹ $(x,y)$ çš„æœ‰å‘çº¿æ®µã€‚\n",
    "<br />\n",
    "<img src=\"./assets/vector.png\" style=\"margin-left: 0px\" width=800px>\n",
    "<br />\n",
    "ä»¥æ­¤ç±»æ¨ï¼Œæˆ‘å¯ä»¥ç”¨ä¸€ç»„åæ ‡ $(x_0, x_1, \\ldots, x_{N-1})$ è¡¨ç¤ºä¸€ä¸ª $N$ ç»´ç©ºé—´ä¸­çš„å‘é‡ï¼Œ$N$ å«å‘é‡çš„ç»´åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 æ–‡æœ¬å‘é‡ï¼ˆText Embeddingsï¼‰\n",
    "\n",
    "1. å°†æ–‡æœ¬è½¬æˆä¸€ç»„ $N$ ç»´æµ®ç‚¹æ•°ï¼Œå³**æ–‡æœ¬å‘é‡**åˆå« Embeddings\n",
    "2. å‘é‡ä¹‹é—´å¯ä»¥è®¡ç®—è·ç¦»ï¼Œè·ç¦»è¿œè¿‘å¯¹åº”**è¯­ä¹‰ç›¸ä¼¼åº¦**å¤§å°\n",
    "\n",
    "<br />\n",
    "<img src=\"./assets/embeddings.png\" style=\"margin-left: 0px\" width=800px>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 æ–‡æœ¬å‘é‡æ˜¯æ€ä¹ˆå¾—åˆ°çš„ï¼ˆé€‰ï¼‰\n",
    "\n",
    "1. æ„å»ºç›¸å…³ï¼ˆæ­£ä¾‹ï¼‰ä¸ä¸ç›¸å…³ï¼ˆè´Ÿä¾‹ï¼‰çš„å¥å­å¯¹æ ·æœ¬\n",
    "2. è®­ç»ƒåŒå¡”å¼æ¨¡å‹ï¼Œè®©æ­£ä¾‹é—´çš„è·ç¦»å°ï¼Œè´Ÿä¾‹é—´çš„è·ç¦»å¤§\n",
    "\n",
    "ä¾‹å¦‚ï¼š\n",
    "\n",
    "<img src=\"./assets/sbert.png\" style=\"margin-left: 0px\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>æ‰©å±•é˜…è¯»ï¼šhttps://www.sbert.net</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 å‘é‡é—´çš„ç›¸ä¼¼åº¦è®¡ç®—\n",
    "\n",
    "<img src=\"./assets/sim.png\" style=\"margin-left: 0px\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    '''ä½™å¼¦è·ç¦» -- è¶Šå¤§è¶Šç›¸ä¼¼'''\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "\n",
    "def l2(a, b):\n",
    "    '''æ¬§æ°è·ç¦» -- è¶Šå°è¶Šç›¸ä¼¼'''\n",
    "    x = np.asarray(a)-np.asarray(b)\n",
    "    return norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-ada-002\", dimensions=None):\n",
    "    '''å°è£… OpenAI çš„ Embedding æ¨¡å‹æ¥å£'''\n",
    "    if model == \"text-embedding-ada-002\":\n",
    "        dimensions = None\n",
    "    if dimensions:\n",
    "        data = client.embeddings.create(\n",
    "            input=texts, model=model, dimensions=dimensions).data\n",
    "    else:\n",
    "        data = client.embeddings.create(input=texts, model=model).data\n",
    "    return [x.embedding for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dimension: 1536\n",
      "First 10 elements: [-0.007304091472178698, -0.006229960359632969, -0.010646641254425049, 0.0014391535660251975, -0.010704899206757545, 0.029274623841047287, -0.019807705655694008, 0.005487171467393637, -0.016865678131580353, -0.011979292146861553]\n"
     ]
    }
   ],
   "source": [
    "test_query = [\"æµ‹è¯•æ–‡æœ¬\"]\n",
    "vec = get_embeddings(test_query)[0]\n",
    "print(f\"Total dimension: {len(vec)}\")\n",
    "print(f\"First 10 elements: {vec[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queryä¸è‡ªå·±çš„ä½™å¼¦è·ç¦»: 1.00\n",
      "Queryä¸Documentsçš„ä½™å¼¦è·ç¦»:\n",
      "0.8224810779975097\n",
      "0.8299968969406545\n",
      "0.798096878742543\n",
      "0.7669367418371253\n",
      "0.7933908049643592\n",
      "\n",
      "Queryä¸è‡ªå·±çš„æ¬§æ°è·ç¦»: 0.00\n",
      "Queryä¸Documentsçš„æ¬§æ°è·ç¦»:\n",
      "0.5958505343143035\n",
      "0.5831005284209486\n",
      "0.6354574886516438\n",
      "0.6827345862824111\n",
      "0.6428206511188914\n"
     ]
    }
   ],
   "source": [
    "query = \"å›½é™…äº‰ç«¯\"\n",
    "\n",
    "# ä¸”èƒ½æ”¯æŒè·¨è¯­è¨€\n",
    "# query = \"global conflicts\"\n",
    "\n",
    "documents = [\n",
    "    \"è”åˆå›½å°±è‹ä¸¹è¾¾å°”å¯Œå°”åœ°åŒºå¤§è§„æ¨¡æš´åŠ›äº‹ä»¶å‘å‡ºè­¦å‘Š\",\n",
    "    \"åœŸè€³å…¶ã€èŠ¬å…°ã€ç‘å…¸ä¸åŒ—çº¦ä»£è¡¨å°†ç»§ç»­å°±ç‘å…¸â€œå…¥çº¦â€é—®é¢˜è¿›è¡Œè°ˆåˆ¤\",\n",
    "    \"æ—¥æœ¬å²é˜œå¸‚é™†ä¸Šè‡ªå«é˜Ÿå°„å‡»åœºå†…å‘ç”Ÿæªå‡»äº‹ä»¶ 3äººå—ä¼¤\",\n",
    "    \"å›½å®¶æ¸¸æ³³ä¸­å¿ƒï¼ˆæ°´ç«‹æ–¹ï¼‰ï¼šæ¢å¤æ¸¸æ³³ã€å¬‰æ°´ä¹å›­ç­‰æ°´ä¸Šé¡¹ç›®è¿è¥\",\n",
    "    \"æˆ‘å›½é¦–æ¬¡åœ¨ç©ºé—´ç«™å¼€å±•èˆ±å¤–è¾å°„ç”Ÿç‰©å­¦æš´éœ²å®éªŒ\",\n",
    "]\n",
    "\n",
    "query_vec = get_embeddings([query])[0]\n",
    "doc_vecs = get_embeddings(documents)\n",
    "\n",
    "print(\"Queryä¸è‡ªå·±çš„ä½™å¼¦è·ç¦»: {:.2f}\".format(cos_sim(query_vec, query_vec)))\n",
    "print(\"Queryä¸Documentsçš„ä½™å¼¦è·ç¦»:\")\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Queryä¸è‡ªå·±çš„æ¬§æ°è·ç¦»: {:.2f}\".format(l2(query_vec, query_vec)))\n",
    "print(\"Queryä¸Documentsçš„æ¬§æ°è·ç¦»:\")\n",
    "for vec in doc_vecs:\n",
    "    print(l2(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 å‘é‡æ•°æ®åº“\n",
    "\n",
    "å‘é‡æ•°æ®åº“ï¼Œæ˜¯ä¸“é—¨ä¸ºå‘é‡æ£€ç´¢è®¾è®¡çš„ä¸­é—´ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œæˆ‘ä»¬åªå–ä¸¤é¡µï¼ˆç¬¬ä¸€ç« ï¼‰\n",
    "paragraphs = extract_text_from_pdf(\n",
    "    \"llama2.pdf\",\n",
    "    page_numbers=[2, 3],\n",
    "    min_line_length=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "class MyVectorDBConnector:\n",
    "    def __init__(self, collection_name, embedding_fn):\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "\n",
    "        # ä¸ºäº†æ¼”ç¤ºï¼Œå®é™…ä¸éœ€è¦æ¯æ¬¡ reset()\n",
    "        chroma_client.reset()\n",
    "\n",
    "        # åˆ›å»ºä¸€ä¸ª collection\n",
    "        self.collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "        self.embedding_fn = embedding_fn\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        '''å‘ collection ä¸­æ·»åŠ æ–‡æ¡£ä¸å‘é‡'''\n",
    "        self.collection.add(\n",
    "            embeddings=self.embedding_fn(documents),  # æ¯ä¸ªæ–‡æ¡£çš„å‘é‡\n",
    "            documents=documents,  # æ–‡æ¡£çš„åŸæ–‡\n",
    "            ids=[f\"id{i}\" for i in range(len(documents))]  # æ¯ä¸ªæ–‡æ¡£çš„ id\n",
    "        )\n",
    "\n",
    "    def search(self, query, top_n):\n",
    "        '''æ£€ç´¢å‘é‡æ•°æ®åº“'''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§\n",
      "\n",
      " In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-speciï¬c data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our ï¬ne-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce ï¬ne-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡\n",
    "vector_db = MyVectorDBConnector(\"demo\", get_embeddings)\n",
    "# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£\n",
    "vector_db.add_documents(paragraphs)\n",
    "\n",
    "user_query = \"Llama 2æœ‰å¤šå°‘å‚æ•°\"\n",
    "# user_query = \"Does Llama 2 have a conversational variant\"\n",
    "results = vector_db.search(user_query, 2)\n",
    "\n",
    "for para in results['documents'][0]:\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>æ¾„æ¸…å‡ ä¸ªå…³é”®æ¦‚å¿µï¼š</b><ul>\n",
    "    <li>å‘é‡æ•°æ®åº“çš„æ„ä¹‰æ˜¯å¿«é€Ÿçš„æ£€ç´¢ï¼›</li>\n",
    "    <li>å‘é‡æ•°æ®åº“æœ¬èº«ä¸ç”Ÿæˆå‘é‡ï¼Œå‘é‡æ˜¯ç”± Embedding æ¨¡å‹äº§ç”Ÿçš„ï¼›</li>\n",
    "    <li>å‘é‡æ•°æ®åº“ä¸ä¼ ç»Ÿçš„å…³ç³»å‹æ•°æ®åº“æ˜¯äº’è¡¥çš„ï¼Œä¸æ˜¯æ›¿ä»£å…³ç³»ï¼Œåœ¨å®é™…åº”ç”¨ä¸­æ ¹æ®å®é™…éœ€æ±‚ç»å¸¸åŒæ—¶ä½¿ç”¨ã€‚</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 å‘é‡æ•°æ®åº“æœåŠ¡\n",
    "\n",
    "Server ç«¯\n",
    "\n",
    "```sh\n",
    "chroma run --path /db_path\n",
    "```\n",
    "\n",
    "Client ç«¯\n",
    "\n",
    "```python\n",
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 ä¸»æµå‘é‡æ•°æ®åº“åŠŸèƒ½å¯¹æ¯”\n",
    "\n",
    "<img src=\"./assets/vectordb.png\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "- FAISS: Meta å¼€æºçš„å‘é‡æ£€ç´¢å¼•æ“ https://github.com/facebookresearch/faiss\n",
    "- Pinecone: å•†ç”¨å‘é‡æ•°æ®åº“ï¼Œåªæœ‰äº‘æœåŠ¡ https://www.pinecone.io/\n",
    "- **Milvus**: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://milvus.io/\n",
    "- Weaviate: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://weaviate.io/\n",
    "- **Qdrant**: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://qdrant.tech/\n",
    "- PGVector: Postgres çš„å¼€æºå‘é‡æ£€ç´¢å¼•æ“ https://github.com/pgvector/pgvector\n",
    "- RediSearch: Redis çš„å¼€æºå‘é‡æ£€ç´¢å¼•æ“ https://github.com/RediSearch/RediSearch\n",
    "- ElasticSearch ä¹Ÿæ”¯æŒå‘é‡æ£€ç´¢ https://www.elastic.co/enterprise-search/vector-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>æ‰©å±•é˜…è¯»ï¼šhttps://guangzhengli.com/blog/zh/vector-database</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 åŸºäºå‘é‡æ£€ç´¢çš„RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_Bot:\n",
    "    def __init__(self, vector_db, llm_api, n_results=2):\n",
    "        self.vector_db = vector_db\n",
    "        self.llm_api = llm_api\n",
    "        self.n_results = n_results\n",
    "\n",
    "    def chat(self, user_query):\n",
    "        # 1. æ£€ç´¢\n",
    "        search_results = self.vector_db.search(user_query, self.n_results)\n",
    "\n",
    "        # 2. æ„å»º Prompt\n",
    "        prompt = build_prompt(\n",
    "            prompt_template, context=search_results['documents'][0], query=user_query)\n",
    "\n",
    "        # 3. è°ƒç”¨ LLM\n",
    "        response = self.llm_api(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2æœ‰7Bã€13Bå’Œ70Bä¸‰ä¸ªå‚æ•°ç‰ˆæœ¬ã€‚\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")\n",
    "\n",
    "user_query = \"llama 2æœ‰å¤šå°‘å‚æ•°?\"\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 OpenAI æ–°å‘å¸ƒçš„ä¸¤ä¸ª Embedding æ¨¡å‹\n",
    "\n",
    "2024 å¹´ 1 æœˆ 25 æ—¥ï¼ŒOpenAI æ–°å‘å¸ƒäº†ä¸¤ä¸ª Embedding æ¨¡å‹\n",
    "\n",
    "- text-embedding-3-large\n",
    "- text-embedding-3-small\n",
    "\n",
    "å…¶æœ€å¤§ç‰¹ç‚¹æ˜¯ï¼Œæ”¯æŒè‡ªå®šä¹‰çš„ç¼©çŸ­å‘é‡ç»´åº¦ï¼Œä»è€Œåœ¨å‡ ä¹ä¸å½±å“æœ€ç»ˆæ•ˆæœçš„æƒ…å†µä¸‹é™ä½å‘é‡æ£€ç´¢ä¸ç›¸ä¼¼åº¦è®¡ç®—çš„å¤æ‚åº¦ã€‚\n",
    "\n",
    "é€šä¿—çš„è¯´ï¼š**è¶Šå¤§è¶Šå‡†ã€è¶Šå°è¶Šå¿«ã€‚** å®˜æ–¹å…¬å¸ƒçš„è¯„æµ‹ç»“æœ:\n",
    "\n",
    "<img src=\"./assets/mteb.png\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "æ³¨ï¼š[MTEB](https://huggingface.co/blog/mteb) æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¤šä»»åŠ¡çš„ Embedding æ¨¡å‹å…¬å¼€è¯„æµ‹é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‘é‡ç»´åº¦: 128\n",
      "\n",
      "Queryä¸Documentsçš„ä½™å¼¦è·ç¦»:\n",
      "0.3341465461188055\n",
      "0.354653344959069\n",
      "0.31360020300733393\n",
      "0.2240495299009075\n",
      "0.12830648514792267\n",
      "\n",
      "Queryä¸Documentsçš„æ¬§æ°è·ç¦»:\n",
      "1.1539960610822753\n",
      "1.13608684105105\n",
      "1.1716653604956542\n",
      "1.245753192131409\n",
      "1.3203738349039935\n"
     ]
    }
   ],
   "source": [
    "model = \"text-embedding-3-large\"\n",
    "dimensions = 128\n",
    "\n",
    "# query = \"å›½é™…äº‰ç«¯\"\n",
    "\n",
    "# ä¸”èƒ½æ”¯æŒè·¨è¯­è¨€\n",
    "query = \"global conflicts\"\n",
    "\n",
    "documents = [\n",
    "    \"è”åˆå›½å°±è‹ä¸¹è¾¾å°”å¯Œå°”åœ°åŒºå¤§è§„æ¨¡æš´åŠ›äº‹ä»¶å‘å‡ºè­¦å‘Š\",\n",
    "    \"åœŸè€³å…¶ã€èŠ¬å…°ã€ç‘å…¸ä¸åŒ—çº¦ä»£è¡¨å°†ç»§ç»­å°±ç‘å…¸â€œå…¥çº¦â€é—®é¢˜è¿›è¡Œè°ˆåˆ¤\",\n",
    "    \"æ—¥æœ¬å²é˜œå¸‚é™†ä¸Šè‡ªå«é˜Ÿå°„å‡»åœºå†…å‘ç”Ÿæªå‡»äº‹ä»¶ 3äººå—ä¼¤\",\n",
    "    \"å›½å®¶æ¸¸æ³³ä¸­å¿ƒï¼ˆæ°´ç«‹æ–¹ï¼‰ï¼šæ¢å¤æ¸¸æ³³ã€å¬‰æ°´ä¹å›­ç­‰æ°´ä¸Šé¡¹ç›®è¿è¥\",\n",
    "    \"æˆ‘å›½é¦–æ¬¡åœ¨ç©ºé—´ç«™å¼€å±•èˆ±å¤–è¾å°„ç”Ÿç‰©å­¦æš´éœ²å®éªŒ\",\n",
    "]\n",
    "\n",
    "query_vec = get_embeddings([query], model=model, dimensions=dimensions)[0]\n",
    "doc_vecs = get_embeddings(documents, model=model, dimensions=dimensions)\n",
    "\n",
    "print(\"å‘é‡ç»´åº¦: {}\".format(len(query_vec)))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Queryä¸Documentsçš„ä½™å¼¦è·ç¦»:\")\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Queryä¸Documentsçš„æ¬§æ°è·ç¦»:\")\n",
    "for vec in doc_vecs:\n",
    "    print(l2(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>æ‰©å±•é˜…è¯»ï¼šè¿™ç§å¯å˜é•¿åº¦çš„ Embedding æŠ€æœ¯èƒŒåçš„åŸç†å«åš <a href=\"https://arxiv.org/abs/2205.13147\">Matryoshka Representation Learning</a> </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å››ã€å®æˆ˜ RAG ç³»ç»Ÿçš„è¿›é˜¶çŸ¥è¯†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 æ–‡æœ¬åˆ†å‰²çš„ç²’åº¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç¼ºé™·**\n",
    "\n",
    "1. ç²’åº¦å¤ªå¤§å¯èƒ½å¯¼è‡´æ£€ç´¢ä¸ç²¾å‡†ï¼Œç²’åº¦å¤ªå°å¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸å…¨é¢\n",
    "2. é—®é¢˜çš„ç­”æ¡ˆå¯èƒ½è·¨è¶Šä¸¤ä¸ªç‰‡æ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡\n",
    "vector_db = MyVectorDBConnector(\"demo_text_split\", get_embeddings)\n",
    "# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£\n",
    "vector_db.add_documents(paragraphs)\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-speciï¬c data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our ï¬ne-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce ï¬ne-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\n",
      "\n",
      " 2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "\n",
      "====å›å¤====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_query = \"llama 2æœ‰å•†ç”¨è®¸å¯åè®®å—\"\n",
    "user_query=\"llama 2 chatæœ‰å¤šå°‘å‚æ•°\"\n",
    "search_results = vector_db.search(user_query, 2)\n",
    "\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "print(\"====å›å¤====\")\n",
    "bot.chat(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Figure 1: Helpfulness human evaluation results for Llama 2-Chat compared to other open-source and closed-source models. Human raters compared model generations on ~4k prompts consisting of both single and multi-turn prompts. The 95% conï¬dence intervals for this evaluation are between 1% and 2%. More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent diï¬ƒculty of comparing generations.\n",
      "\n",
      " Figure 2: Win-rate % for helpfulness andsafety between commercial-licensed baselines and Llama 2-Chat, according to GPT 4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.\n",
      "\n",
      " 1 Introduction\n",
      "\n",
      " Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of ï¬elds, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.\n",
      "\n",
      " The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoï¬€mann et al., 2022), but none of these models are suitable substitutes for closed â€œproductâ€ LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily ï¬ne-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require signiï¬cant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.\n",
      "\n",
      " In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-speciï¬c data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our ï¬ne-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce ï¬ne-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\n",
      "\n",
      "Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models. Human raters judged model generations for safety violations across ~2,000 adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models.\n",
      "\n",
      " We are releasing the following models to the general public for research and commercial useâ€¡:\n",
      "\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§\n",
      "\n",
      " 2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "\n",
      " variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "\n",
      " We believe that the open release of LLMs, when done safely, will be a net beneï¬t to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their speciï¬c applications of the model. We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      " The remainder of this paper describes our pretraining methodology (Section 2), ï¬ne-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).\n",
      "\n",
      " â€¡https://ai.meta.com/resources/models-and-libraries/llama/ Â§We are delaying the release of the 34B model due to a lack of time to suï¬ƒciently red team. Â¶https://ai.meta.com/llama â€–https://github.com/facebookresearch/llama\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in paragraphs:\n",
    "    print(p+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ”¹è¿›**: æŒ‰ä¸€å®šç²’åº¦ï¼Œéƒ¨åˆ†é‡å å¼çš„åˆ‡å‰²æ–‡æœ¬ï¼Œä½¿ä¸Šä¸‹æ–‡æ›´å®Œæ•´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "\n",
    "\n",
    "def split_text(paragraphs, chunk_size=300, overlap_size=100):\n",
    "    '''æŒ‰æŒ‡å®š chunk_size å’Œ overlap_size äº¤å å‰²æ–‡æœ¬'''\n",
    "    sentences = [s.strip() for p in paragraphs for s in sent_tokenize(p)]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        chunk = sentences[i]\n",
    "        overlap = ''\n",
    "        prev_len = 0\n",
    "        prev = i - 1\n",
    "        # å‘å‰è®¡ç®—é‡å éƒ¨åˆ†\n",
    "        while prev >= 0 and len(sentences[prev])+len(overlap) <= overlap_size:\n",
    "            overlap = sentences[prev] + ' ' + overlap\n",
    "            prev -= 1\n",
    "        chunk = overlap+chunk\n",
    "        next = i + 1\n",
    "        # å‘åè®¡ç®—å½“å‰chunk\n",
    "        while next < len(sentences) and len(sentences[next])+len(chunk) <= chunk_size:\n",
    "            chunk = chunk + ' ' + sentences[next]\n",
    "            next += 1\n",
    "        chunks.append(chunk)\n",
    "        i = next\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "æ­¤å¤„ sent_tokenize ä¸ºé’ˆå¯¹è‹±æ–‡çš„å®ç°ï¼Œé’ˆå¯¹ä¸­æ–‡çš„å®ç°è¯·å‚è€ƒ chinese_utils.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(paragraphs, 300, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡\n",
    "vector_db = MyVectorDBConnector(\"demo_text_split\", get_embeddings)\n",
    "# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£\n",
    "vector_db.add_documents(chunks)\n",
    "# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net beneï¬t to society.\n",
      "\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.\n",
      "\n",
      "====å›å¤====\n",
      "Llama 2-Chatæœ‰7Bã€13Bå’Œ70Bä¸‰ç§å‚æ•°è§„æ¨¡ã€‚\n"
     ]
    }
   ],
   "source": [
    "# user_query = \"llama 2æœ‰å•†ç”¨è®¸å¯åè®®å—\"\n",
    "user_query=\"llama 2 chatæœ‰å¤šå°‘å‚æ•°\"\n",
    "\n",
    "search_results = vector_db.search(user_query, 2)\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(\"====å›å¤====\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 æ£€ç´¢åæ’åº"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**é—®é¢˜**: æœ‰æ—¶ï¼Œæœ€åˆé€‚çš„ç­”æ¡ˆä¸ä¸€å®šæ’åœ¨æ£€ç´¢çš„æœ€å‰é¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"how safe is llama 2\"\n",
    "search_results = vector_db.search(user_query, 5)\n",
    "\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(\"====å›å¤====\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ–¹æ¡ˆ**:\n",
    "\n",
    "1. æ£€ç´¢æ—¶è¿‡æ‹›å›ä¸€éƒ¨åˆ†æ–‡æœ¬\n",
    "2. é€šè¿‡ä¸€ä¸ªæ’åºæ¨¡å‹å¯¹ query å’Œ document é‡æ–°æ‰“åˆ†æ’åº\n",
    "\n",
    "<img src=\"./assets/sbert-rerank.png\" style=\"margin-left: 0px\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512) # è‹±æ–‡ï¼Œæ¨¡å‹è¾ƒå°\n",
    "model = CrossEncoder('BAAI/bge-reranker-large', max_length=512) # å¤šè¯­è¨€ï¼Œå›½äº§ï¼Œæ¨¡å‹è¾ƒå¤§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"how safe is llama 2\"\n",
    "# user_query = \"llama 2å®‰å…¨æ€§å¦‚ä½•\"\n",
    "scores = model.predict([(user_query, doc)\n",
    "                       for doc in search_results['documents'][0]])\n",
    "# æŒ‰å¾—åˆ†æ’åº\n",
    "sorted_list = sorted(\n",
    "    zip(scores, search_results['documents'][0]), key=lambda x: x[0], reverse=True)\n",
    "for score, doc in sorted_list:\n",
    "    print(f\"{score}\\t{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä¸€äº› Rerank çš„ API æœåŠ¡ \n",
    "\n",
    "- [Cohere Rerank](https://cohere.com/rerank)ï¼šæ”¯æŒå¤šè¯­è¨€\n",
    "- [Jina Rerank](https://jina.ai/reranker/)ï¼šç›®å‰åªæ”¯æŒè‹±æ–‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äº”ã€PDF æ–‡æ¡£ä¸­çš„è¡¨æ ¼æ€ä¹ˆå¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/table_rag.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. å°†æ¯é¡µ PDF è½¬æˆå›¾ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyMuPDF\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "from PIL import Image\n",
    "\n",
    "def pdf2images(pdf_file):\n",
    "    '''å°† PDF æ¯é¡µè½¬æˆä¸€ä¸ª PNG å›¾åƒ'''\n",
    "    # ä¿å­˜è·¯å¾„ä¸ºåŸ PDF æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰\n",
    "    output_directory_path, _ = os.path.splitext(pdf_file)\n",
    "    \n",
    "    if not os.path.exists(output_directory_path):\n",
    "        os.makedirs(output_directory_path)\n",
    "    \n",
    "    # åŠ è½½ PDF æ–‡ä»¶\n",
    "    pdf_document = fitz.open(pdf_file)\n",
    "    \n",
    "    # æ¯é¡µè½¬ä¸€å¼ å›¾\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        # å–ä¸€é¡µ\n",
    "        page = pdf_document[page_number]\n",
    "    \n",
    "        # è½¬å›¾åƒ\n",
    "        pix = page.get_pixmap()\n",
    "    \n",
    "        # ä»ä½å›¾åˆ›å»º PNG å¯¹è±¡\n",
    "        image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    \n",
    "        # ä¿å­˜ PNG æ–‡ä»¶\n",
    "        image.save(f\"./{output_directory_path}/page_{page_number + 1}.png\")\n",
    "    \n",
    "    # å…³é—­ PDF æ–‡ä»¶\n",
    "    pdf_document.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(dir_path):\n",
    "    '''æ˜¾ç¤ºç›®å½•ä¸‹çš„ PNG å›¾åƒ'''\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith('.png'):\n",
    "            # æ‰“å¼€å›¾åƒ\n",
    "            img = Image.open(os.path.join(dir_path, file)) \n",
    "\n",
    "            # æ˜¾ç¤ºå›¾åƒ\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')  # ä¸æ˜¾ç¤ºåæ ‡è½´\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2images(\"llama2_page8.pdf\")\n",
    "show_images(\"llama2_page8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. è¯†åˆ«æ–‡æ¡£ï¼ˆå›¾ç‰‡ï¼‰ä¸­çš„è¡¨æ ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxResize(object):\n",
    "    '''ç¼©æ”¾å›¾åƒ'''\n",
    "    def __init__(self, max_size=800):\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        current_max_size = max(width, height)\n",
    "        scale = self.max_size / current_max_size\n",
    "        resized_image = image.resize(\n",
    "            (int(round(scale * width)), int(round(scale * height)))\n",
    "        )\n",
    "\n",
    "        return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# å›¾åƒé¢„å¤„ç†\n",
    "detection_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(800),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "# åŠ è½½ TableTransformer æ¨¡å‹\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-transformer-detection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯†åˆ«åçš„åæ ‡æ¢ç®—ä¸åå¤„ç†\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    '''åæ ‡è½¬æ¢'''\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    '''åŒºåŸŸç¼©æ”¾'''\n",
    "    width, height = size\n",
    "    boxes = box_cxcywh_to_xyxy(out_bbox)\n",
    "    boxes = boxes * torch.tensor(\n",
    "        [width, height, width, height], dtype=torch.float32\n",
    "    )\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    '''ä»æ¨¡å‹è¾“å‡ºä¸­å–å®šä½æ¡†åæ ‡'''\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
    "    pred_bboxes = [\n",
    "        elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)\n",
    "    ]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if not class_label == \"no object\":\n",
    "            objects.append(\n",
    "                {\n",
    "                    \"label\": class_label,\n",
    "                    \"score\": float(score),\n",
    "                    \"bbox\": [float(elem) for elem in bbox],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# è¯†åˆ«è¡¨æ ¼ï¼Œå¹¶å°†è¡¨æ ¼éƒ¨åˆ†å•ç‹¬å­˜ä¸ºå›¾åƒæ–‡ä»¶\n",
    "\n",
    "def detect_and_crop_save_table(file_path):\n",
    "    # åŠ è½½å›¾åƒï¼ˆPDFé¡µï¼‰    \n",
    "    image = Image.open(file_path)\n",
    "\n",
    "    filename, _ = os.path.splitext(os.path.basename(file_path))\n",
    "\n",
    "    # è¾“å‡ºè·¯å¾„\n",
    "    cropped_table_directory = os.path.join(os.path.dirname(file_path), \"table_images\")\n",
    "\n",
    "    if not os.path.exists(cropped_table_directory):\n",
    "        os.makedirs(cropped_table_directory)\n",
    "\n",
    "    # é¢„å¤„ç†\n",
    "    pixel_values = detection_transform(image).unsqueeze(0)\n",
    "\n",
    "    # è¯†åˆ«è¡¨æ ¼\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "    # åå¤„ç†ï¼Œå¾—åˆ°è¡¨æ ¼å­åŒºåŸŸ\n",
    "    id2label = model.config.id2label\n",
    "    id2label[len(model.config.id2label)] = \"no object\"\n",
    "    detected_tables = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    print(f\"number of tables detected {len(detected_tables)}\")\n",
    "\n",
    "    for idx in range(len(detected_tables)):\n",
    "        # å°†è¯†åˆ«ä»çš„è¡¨æ ¼åŒºåŸŸå•ç‹¬å­˜ä¸ºå›¾åƒ\n",
    "        cropped_table = image.crop(detected_tables[idx][\"bbox\"])\n",
    "        cropped_table.save(os.path.join(cropped_table_directory,f\"{filename}_{idx}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_and_crop_save_table(\"llama2_page8/page_1.png\")\n",
    "show_images(\"llama2_page8/table_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. åŸºäº GPT-4 Vision API åšè¡¨æ ¼é—®ç­”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def image_qa(query, image_path):\n",
    "    base64_image = encode_image(image_path)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        seed=42,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                  {\"type\": \"text\", \"text\": query},\n",
    "                  {\n",
    "                      \"type\": \"image_url\",\n",
    "                      \"image_url\": {\n",
    "                          \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                      },\n",
    "                  },\n",
    "              ],\n",
    "        }],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = image_qa(\"å“ªä¸ªæ¨¡å‹åœ¨AGI Evalæ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½ã€‚å¾—åˆ†å¤šå°‘\",\"llama2_page8/table_images/page_1_0.png\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ç”¨ GPT-4 Vision ç”Ÿæˆè¡¨æ ¼ï¼ˆå›¾åƒï¼‰æè¿°ï¼Œå¹¶å‘é‡åŒ–ç”¨äºæ£€ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "class NewVectorDBConnector:\n",
    "    def __init__(self, collection_name, embedding_fn):\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "\n",
    "        # ä¸ºäº†æ¼”ç¤ºï¼Œå®é™…ä¸éœ€è¦æ¯æ¬¡ reset()\n",
    "        chroma_client.reset()\n",
    "\n",
    "        # åˆ›å»ºä¸€ä¸ª collection\n",
    "        self.collection = chroma_client.get_or_create_collection(\n",
    "            name=collection_name)\n",
    "        self.embedding_fn = embedding_fn\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        '''å‘ collection ä¸­æ·»åŠ æ–‡æ¡£ä¸å‘é‡'''\n",
    "        self.collection.add(\n",
    "            embeddings=self.embedding_fn(documents),  # æ¯ä¸ªæ–‡æ¡£çš„å‘é‡\n",
    "            documents=documents,  # æ–‡æ¡£çš„åŸæ–‡\n",
    "            ids=[f\"id{i}\" for i in range(len(documents))]  # æ¯ä¸ªæ–‡æ¡£çš„ id\n",
    "        )\n",
    "\n",
    "    def add_images(self, image_paths):\n",
    "        '''å‘ collection ä¸­æ·»åŠ å›¾åƒ'''\n",
    "        documents = [\n",
    "            image_qa(\"è¯·ç®€è¦æè¿°å›¾ç‰‡ä¸­çš„ä¿¡æ¯\",image)\n",
    "            for image in image_paths\n",
    "        ]\n",
    "        self.collection.add(\n",
    "            embeddings=self.embedding_fn(documents),  # æ¯ä¸ªæ–‡æ¡£çš„å‘é‡\n",
    "            documents=documents,  # æ–‡æ¡£çš„åŸæ–‡\n",
    "            ids=[f\"id{i}\" for i in range(len(documents))],  # æ¯ä¸ªæ–‡æ¡£çš„ id\n",
    "            metadatas=[{\"image\": image} for image in image_paths] # ç”¨ metadata æ ‡è®°æºå›¾åƒè·¯å¾„\n",
    "        )\n",
    "\n",
    "    def search(self, query, top_n):\n",
    "        '''æ£€ç´¢å‘é‡æ•°æ®åº“'''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "dir_path = \"llama2_page8/table_images\"\n",
    "for file in os.listdir(dir_path):\n",
    "    if file.endswith('.png'):\n",
    "        # æ‰“å¼€å›¾åƒ\n",
    "        images.append(os.path.join(dir_path, file))\n",
    "\n",
    "new_db_connector = NewVectorDBConnector(\"table_demo\",get_embeddings)\n",
    "new_db_connector.add_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = \"å“ªä¸ªæ¨¡å‹åœ¨AGI Evalæ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½ã€‚å¾—åˆ†å¤šå°‘\"\n",
    "\n",
    "results = new_db_connector.search(query, 1)\n",
    "metadata = results[\"metadatas\"][0]\n",
    "print(\"====æ£€ç´¢ç»“æœ====\")\n",
    "print(metadata)\n",
    "print(\"====å›å¤====\")\n",
    "response = image_qa(query,metadata[0][\"image\"])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€äº›é¢å‘ RAG çš„æ–‡æ¡£è§£æè¾…åŠ©å·¥å…·\n",
    "\n",
    "- [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/): PDF æ–‡ä»¶å¤„ç†åŸºç¡€åº“ï¼Œå¸¦æœ‰åŸºäºè§„åˆ™çš„è¡¨æ ¼ä¸å›¾åƒæŠ½å–ï¼ˆä¸å‡†ï¼‰\n",
    "- [RAGFlow](https://github.com/infiniflow/ragflow): ä¸€æ¬¾åŸºäºæ·±åº¦æ–‡æ¡£ç†è§£æ„å»ºçš„å¼€æº RAG å¼•æ“ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼\n",
    "- [Unstructured.io](https://unstructured.io/): ä¸€ä¸ªå¼€æº+SaaSå½¢å¼çš„æ–‡æ¡£è§£æåº“ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼\n",
    "- [LlamaParse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/)ï¼šä»˜è´¹ API æœåŠ¡ï¼Œç”± LlamaIndex å®˜æ–¹æä¾›ï¼Œè§£æä¸ä¿è¯100%å‡†ç¡®ï¼Œå®æµ‹å¶æœ‰æ–‡å­—ä¸¢å¤±æˆ–é”™ä½å‘ç”Ÿ\n",
    "- [Mathpix](https://mathpix.com/)ï¼šä»˜è´¹ API æœåŠ¡ï¼Œæ•ˆæœè¾ƒå¥½ï¼Œå¯è§£ææ®µè½ç»“æ„ã€è¡¨æ ¼ã€å…¬å¼ç­‰ï¼Œè´µï¼\n",
    "\n",
    "åœ¨å·¥ç¨‹ä¸Šï¼ŒPDF è§£ææœ¬èº«æ˜¯ä¸ªå¤æ‚ä¸”çç¢çš„å·¥ä½œã€‚ä»¥ä¸Šå·¥å…·éƒ½ä¸å®Œç¾ï¼Œå»ºè®®åœ¨è‡ªå·±å®é™…åœºæ™¯æµ‹è¯•åé€‰æ‹©ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…­ã€è¯´è¯´ GraphRAG\n",
    "\n",
    "<img src=\"./assets/GraphRAG.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **ä»€ä¹ˆæ˜¯ GraphRAG**ï¼šæ ¸å¿ƒæ€æƒ³æ˜¯å°†çŸ¥è¯†é¢„å…ˆå¤„ç†æˆçŸ¥è¯†å›¾è°±\n",
    "2. **ä¼˜ç‚¹**ï¼šé€‚åˆå¤æ‚é—®é¢˜ï¼Œå°¤å…¶æ˜¯ä»¥æŸ¥è¯¢ä¸ºä¸­å¿ƒçš„æ€»ç»“ï¼Œä¾‹å¦‚ï¼šâ€œXXXå›¢é˜Ÿå»å¹´æœ‰å“ªäº›è´¡çŒ®â€\n",
    "3. **ç¼ºç‚¹**ï¼šçŸ¥è¯†å›¾è°±çš„æ„å»ºã€æ¸…æ´—ã€ç»´æŠ¤æ›´æ–°ç­‰éƒ½æœ‰å¯è§‚çš„æˆæœ¬\n",
    "4. **å»ºè®®**ï¼š\n",
    "   - GraphRAG ä¸æ˜¯ä¸‡èƒ½è‰¯è¯\n",
    "   - é¢†ä¼šå…¶æ ¸å¿ƒæ€æƒ³\n",
    "   - é‡åˆ°ä¼ ç»Ÿ RAG æ— è®ºå¦‚ä½•ä¼˜åŒ–éƒ½ä¸å¥½è§£å†³çš„é—®é¢˜æ—¶ï¼Œé…Œæƒ…ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "### RAG çš„æµç¨‹\n",
    "\n",
    "- ç¦»çº¿æ­¥éª¤ï¼š\n",
    "  1. æ–‡æ¡£åŠ è½½\n",
    "  2. æ–‡æ¡£åˆ‡åˆ†\n",
    "  3. å‘é‡åŒ–\n",
    "  4. çŒå…¥å‘é‡æ•°æ®åº“\n",
    "- åœ¨çº¿æ­¥éª¤ï¼š\n",
    "  1. è·å¾—ç”¨æˆ·é—®é¢˜\n",
    "  2. ç”¨æˆ·é—®é¢˜å‘é‡åŒ–\n",
    "  3. æ£€ç´¢å‘é‡æ•°æ®åº“\n",
    "  4. å°†æ£€ç´¢ç»“æœå’Œç”¨æˆ·é—®é¢˜å¡«å…¥ Prompt æ¨¡ç‰ˆ\n",
    "  5. ç”¨æœ€ç»ˆè·å¾—çš„ Prompt è°ƒç”¨ LLM\n",
    "  6. ç”± LLM ç”Ÿæˆå›å¤\n",
    "\n",
    "### æˆ‘ç”¨äº†ä¸€ä¸ªå¼€æºçš„ RAGï¼Œä¸å¥½ä½¿æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "1. æ£€æŸ¥é¢„å¤„ç†æ•ˆæœï¼šæ–‡æ¡£åŠ è½½æ˜¯å¦æ­£ç¡®ï¼Œåˆ‡å‰²çš„æ˜¯å¦åˆç†\n",
    "2. æµ‹è¯•æ£€ç´¢æ•ˆæœï¼šé—®é¢˜æ£€ç´¢å›æ¥çš„æ–‡æœ¬ç‰‡æ®µæ˜¯å¦åŒ…å«ç­”æ¡ˆ\n",
    "3. æµ‹è¯•å¤§æ¨¡å‹èƒ½åŠ›ï¼šç»™å®šé—®é¢˜å’ŒåŒ…å«ç­”æ¡ˆæ–‡æœ¬ç‰‡æ®µçš„å‰æä¸‹ï¼Œå¤§æ¨¡å‹èƒ½ä¸èƒ½æ­£ç¡®å›ç­”é—®é¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½œä¸š\n",
    "\n",
    "åšä¸ªè‡ªå·±çš„ ChatPDFã€‚éœ€æ±‚ï¼š\n",
    "\n",
    "1. ä»æœ¬åœ°åŠ è½½ PDF æ–‡ä»¶ï¼ŒåŸºäº PDF çš„å†…å®¹å¯¹è¯\n",
    "2. å¯ä»¥æ— å‰ç«¯ï¼Œåªè¦èƒ½åœ¨å‘½ä»¤è¡Œè¿è¡Œå°±è¡Œ\n",
    "3. å…¶å®ƒéšæ„å‘æŒ¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
